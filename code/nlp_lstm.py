# -*- coding: utf-8 -*-
"""Submission_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4bRdmE8bT8M9N-mW3Kc48DN55nuW1R2
"""

from google.colab import files
!pip install -q kaggle

#upload api key
uploaded = files.upload()

! mkdir ~/.kaggle/
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d lokkagle/movie-genre-data
! unzip movie-genre-data.zip

#all import
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
import matplotlib.pyplot as plt
import re
import string
import nltk
import itertools
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize

df = pd.read_csv("kaggle_movie_train.csv")
df

#check genre yang ada
df['genre'].value_counts()

#drop id
df = df.drop(columns=['id'])
#ambol 3 genre saja comedy,action, other(sci_fi+horror+adventure,romance)
df['genre'].replace({"sci-fi":"other",
                     "horror":"other",
                     "adventure":"other",
                     "romance":"other"}, inplace=True)
#hanya mengambil 3 genre
df = df[~df['genre'].isin(['drama','thriller'])]
df

#check value masing masing genre
df['genre'].value_counts()

"""DATA CLEAN"""

def case_fold(text):
  #remove number
  text = re.sub(r"\d+", "", str(text))
  #Menghapus HTTP yang tidak selesai
  text = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
  #menghapus kata-kata ganda
  text = re.sub(r"\b(\w+)(?:\W\1\b)+", r"\1", text, flags=re.IGNORECASE)
  # remove tab, new line, ans back slice
  text = text.replace('\\t'," ").replace('\\n'," ").replace('\\u'," ").replace('\\',"")
  # remove non ASCII (emoticon, chinese word, .etc)
  text = text.encode('ascii', 'replace').decode('ascii')
  # remove mention, link, hashtag
  text = ' '.join(re.sub("([@#][A-Za-z0-9]+)|(\w+:\/\/\S+)"," ", text).split())
  # remove incomplete URL
  text.replace("http://", " ").replace("https://", " ")
  #remove punctuation
  text = text.translate(str.maketrans("","",string.punctuation))
  #remove whitestrip
  text = text.strip()
  #remove multiple whitespace into single whitespace
  text = re.sub('\s+',' ',text)
  #remove single char
  text = re.sub(r"\b[a-zA-Z]\b", "", text)
  #remove url
  url_pattern = re.compile(r'https?://\S+|www\.\S+')
  text = url_pattern.sub(r'', text)
  #duplicate alphabet
  text = ''.join(alp for alp, _ in itertools.groupby(text))
  #membuat semua komentar menjadi huruf kecil 
  text = text.lower()
  return text
df['Case_Fold'] = df['text'].apply(case_fold)

#stopwords english
nltk.download('stopwords')
stop = stopwords.words('english')
df['Stopwords'] = df['Case_Fold'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
df

#lema
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
df['Lemma']= df['Stopwords'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()  ]))
df

#membuat dataframe hasil data clean
df = df.drop(columns=['text','Case_Fold','Stopwords'])
df = df.reindex(['Lemma','genre'], axis=1)
df = df.rename(columns={"Lemma": "text"})
df

"""ONE HOT ENCODING

"""

genre = pd.get_dummies(df.genre)
df_new = pd.concat([df,genre],axis=1)
df_new = df_new.drop(columns=['genre'])
df_new

text = df_new['text'].astype(str)
label = df_new[['action','comedy','other']].values

#split data
X_train,X_test,y_train,y_test = train_test_split(text,label, test_size=0.2)

#TOKENIZER
tokenizer = Tokenizer(num_words=8000, oov_token='x')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

sekuens_train = tokenizer.texts_to_sequences(X_train)
sekuens_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sekuens_train)
padded_test = pad_sequences(sekuens_test)

model = tf.keras.Sequential([
                             tf.keras.layers.Embedding(input_dim=8000, output_dim=16),
                             tf.keras.layers.LSTM(64),

                             tf.keras.layers.Dense(128,activation='relu'),
                             tf.keras.layers.Dense(64,activation='relu'),
                             tf.keras.layers.Dense(32,activation='relu'),
                             
                             tf.keras.layers.Dropout(0.5),
                             tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.90 ):
      print("\nAkurasi telah mencapai >85%!")
      self.model.stop_training = True
callbacks = myCallback()

history = model.fit(padded_train, 
                    y_train, 
                    epochs=30, 
                    callbacks=[callbacks],
                    batch_size=64,
                    validation_data=(padded_test, y_test), 
                    verbose=2)

model.evaluate(padded_test,y_test)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()